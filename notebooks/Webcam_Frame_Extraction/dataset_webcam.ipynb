{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "#from transformers import pipeline\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "import io\n",
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "image_processor_tiny = AutoImageProcessor.from_pretrained(\"hustvl/yolos-tiny\")\n",
    "model_tiny = AutoModelForObjectDetection.from_pretrained(\"hustvl/yolos-tiny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "COLORS = [\"#ff7f7f\", \"#ff7fbf\", \"#ff7fff\", \"#bf7fff\",\n",
    "            \"#7f7fff\", \"#7fbfff\", \"#7fffff\", \"#7fffbf\",\n",
    "            \"#7fff7f\", \"#bfff7f\", \"#ffff7f\", \"#ffbf7f\"]\n",
    "\n",
    "fdic = {\n",
    "    \"family\" : \"DejaVu Serif\",\n",
    "    \"style\" : \"normal\",\n",
    "    \"size\" : 18,\n",
    "    \"color\" : \"yellow\",\n",
    "    \"weight\" : \"bold\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_figure(in_pil_img, in_results):\n",
    "    plt.figure(figsize=(16, 10))\n",
    "    plt.imshow(in_pil_img)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for score, label, box in zip(in_results[\"scores\"], in_results[\"labels\"], in_results[\"boxes\"]):\n",
    "        selected_color = choice(COLORS)\n",
    "\n",
    "        box_int = [i.item() for i in torch.round(box).to(torch.int32)]\n",
    "        x, y, w, h = box_int[0], box_int[1], box_int[2]-box_int[0], box_int[3]-box_int[1]\n",
    "        #x, y, w, h = torch.round(box[0]).item(), torch.round(box[1]).item(), torch.round(box[2]-box[0]).item(), torch.round(box[3]-box[1]).item()\n",
    "\n",
    "        ax.add_patch(plt.Rectangle((x, y), w, h, fill=False, color=selected_color, linewidth=3, alpha=0.8))\n",
    "        ax.text(x, y, f\"{model_tiny.config.id2label[label.item()]}: {round(score.item()*100, 2)}%\", fontdict=fdic, alpha=0.8)\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    # return plt.gcf()\n",
    "def infer(in_pil_img, in_model=\"yolos-tiny\", in_threshold=0.9):\n",
    "    target_sizes = torch.tensor([in_pil_img.size[::-1]])\n",
    "\n",
    "    if in_model == \"yolos-small\":\n",
    "        inputs = image_processor_small(images=in_pil_img, return_tensors=\"pt\")\n",
    "        outputs = model_small(**inputs)\n",
    "\n",
    "        # convert outputs (bounding boxes and class logits) to COCO API\n",
    "        results = image_processor_small.post_process_object_detection(outputs, threshold=in_threshold, target_sizes=target_sizes)[0]\n",
    "\n",
    "    else:\n",
    "        inputs = image_processor_tiny(images=in_pil_img, return_tensors=\"pt\")\n",
    "        outputs = model_tiny(**inputs)\n",
    "\n",
    "        # convert outputs (bounding boxes and class logits) to COCO API\n",
    "        results = image_processor_tiny.post_process_object_detection(outputs, threshold=in_threshold, target_sizes=target_sizes)[0]\n",
    "\n",
    "    # figure = get_figure(in_pil_img, results)\n",
    "\n",
    "    # buf = io.BytesIO()\n",
    "    # figure.savefig(buf, bbox_inches='tight')\n",
    "    # buf.seek(0)\n",
    "    # output_pil_img = Image.open(buf)\n",
    "\n",
    "    # return output_pil_img, results\n",
    "    return results\n",
    "\n",
    "def count_cars(img, threshold=0.6):\n",
    "    Results = []\n",
    "    for i in range(0,4):\n",
    "        for j in range(0,4):\n",
    "            img_arr = np.array(img)[180*i:180+180*i,320*j:320+320*j,:3]\n",
    "            img_new = Image.fromarray(img_arr)\n",
    "            results = infer(img_new, in_model=\"yolos-tiny\", in_threshold=threshold)\n",
    "            Results.append(results)\n",
    "    count = []\n",
    "    for in_results in Results:\n",
    "        count.extend([i.item() for i in in_results['labels'] if i==3 or i == 8])\n",
    "\n",
    "    return len(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_path = \"/home/akansh-i2sc/Desktop/Project/detect-car-LR/data/Webcam/Frames/\"\n",
    "frame_list = os.listdir(frame_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_cars(Image.open(frame_path+frame_list[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
